# -*- coding: utf-8 -*-
"""apple_stock_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OQsDyjSWqalQJf6iqIHU4VsKyOHmWi39
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd

# visualizations
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import plotly
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from pylab import rcParams
rcParams['figure.figsize'] = (20,8)

# time series related 
import statsmodels.api as sm
# !pip install pmdarima
from pmdarima import auto_arima
import statsmodels.api as sm
import statsmodels.graphics.api as smg
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import acf, pacf
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.arima_model import ARMA,ARIMA
import math
from sklearn.metrics import mean_squared_error , mean_absolute_error , mean_absolute_percentage_error
from statsmodels.graphics.tsaplots import plot_predict
from sklearn import svm
from sklearn.svm import SVR

# handle warnings
import warnings
warnings.filterwarnings(action='ignore',category=DeprecationWarning)
warnings.filterwarnings(action='ignore',category=FutureWarning)

df = pd.read_csv('/content/apple_price.csv')
df

# As objects
df.info()

# Check for missing data
df.isnull().sum()

# Drop the existing duplicates
df.drop_duplicates()

# Data Modularity
df.describe()

apple_df = df.copy()

# Change date column type
apple_df['Date'] = pd.to_datetime(apple_df['Date'])
type(apple_df.loc[0,'Date'])

apple_df['year'] = apple_df['Date'].apply(lambda datetime: datetime.year)
apple_df['month'] = apple_df['Date'].apply(lambda datetime: datetime.month_name())
apple_df['day_name'] = apple_df['Date'].apply(lambda datetime: datetime.day_name())
apple_df.set_index("Date", inplace=True)
apple_df.head()

"""## Exploratory Data Analysis - EDA"""

fig = make_subplots(rows=4, cols=1,subplot_titles=['Open', 'High', 'Low','Close'])

fig.add_trace(go.Scatter(x=apple_df.index, y=apple_df['Open'], name='Open',
                        line=dict(color='deepskyblue')),row=1, col=1)

fig.add_trace(go.Scatter(x=apple_df.index, y=apple_df['High'], name='High',
                        line=dict(color='salmon')),row=2,col=1)

fig.add_trace(go.Scatter(x=apple_df.index, y=apple_df['Low'], 
                         name='Low',line=dict(color='lightgreen')),row=3, col=1)

fig.add_trace(go.Scatter(x=apple_df.index, y=apple_df['Close'], 
                         name='Low',line=dict(color='black')),row=4, col=1)

fig.update_layout(showlegend=False,title_text="APPLE STOCK PRICES BY YEAR", title_x=0.5,
                  height=1200, width=800,
        font=dict(
            family="Courier New, monospace",
            size=14,
            color="black"
))

"""### There is a constant trend in apple stocks as well as increasing trend but we could also see that the stocks are down in the early 2022"""

# Investigate on the decline
years_after_2010 =  apple_df[apple_df.index.year > 2010]


fig = make_subplots(rows=4, cols=1,subplot_titles=['Open', 'High', 'Low', 'Close'])

fig.add_trace(go.Scatter(x=years_after_2010.index, y=years_after_2010['Open'], mode='lines', 
                         line=dict(color='deepskyblue')), row=1, col=1)

fig.add_trace(go.Scatter(x=years_after_2010.index, y=years_after_2010['High'], mode='lines',
                        line=dict(color='salmon')), row=2, col=1)

fig.add_trace(go.Scatter(x=years_after_2010.index, y=years_after_2010['Low'], mode='lines',
                        line=dict(color='lightgreen')), row=3, col=1)

fig.add_trace(go.Scatter(x=years_after_2010.index, y=years_after_2010['Close'], mode='lines',
                        line=dict(color='black')), row=4, col=1)

fig.update_layout(showlegend=False,title_text="APPLE STOCK PRICES FROM 2011 TO PRESENT", title_x=0.5,
                  height=1200, width=800,
        font=dict(
            family="Courier New, monospace",
            size=14,
            color="black"
))

# Prediction using Open-High-Low-Close Chart
fig = go.Figure(data=go.Ohlc(x=years_after_2010.index, open=years_after_2010["Open"], high=years_after_2010["High"], low=years_after_2010["Low"], close=years_after_2010["Close"]))
fig.update(layout_xaxis_rangeslider_visible=False)
fig.update_layout(title_text="O-H-L-C Chart", title_x=0.5)
fig.show()

# Determine the volume of shares
fig = go.Figure()

fig.add_trace(go.Scatter(x=apple_df.index, y=apple_df['Volume'].values, 
                         line=dict(color='plum')))

fig.update_layout(showlegend=False,title_text="VOLUME OF SHARES", title_x=0.5,
                  height=500, width=900,
        font=dict(
            family="Courier New, monospace",
            size=14,
            color="black"
))

# Highest volume of share is in September 2000 of about 7.42B

# Determine the volume of shares from 2010-present
volume_2010_now = apple_df[apple_df.year > 2010]

fig = go.Figure()
fig.add_trace(go.Scatter(x=volume_2010_now.index, y=volume_2010_now['Volume'].values,
                        line = dict(color='plum')))

fig.update_layout(showlegend=False,title_text="VOLUME OF SHARES FROM 2010 TO PRESENT", title_x=0.5,
                  height=500, width=900,
        font=dict(
            family="Courier New, monospace",
            size=14,
            color="black"
))

# Volumes of shares have been declining over the past 12 years from the trend by month and this doesn't look to be good.

volume_per_year = apple_df.groupby("year")[["Volume"]].sum()
print(volume_per_year)

fig = px.line(volume_per_year, x=volume_per_year.index, y='Volume', title = 'AVERAGE VOLUME OF SHARES BY YEAR')
fig.show()

# Higher volume of shares was recorded in 2008 estimated to be about 285.98B. This has been the better year in terms of the Apple's movements

# Volume of shares by month to see the right month for investing
volume_per_month = apple_df.groupby("month")[["Volume"]].sum()
print(volume_per_month)

fig = px.line(volume_per_month, x=volume_per_month.index, y='Volume', title='AVERAGE VOLUME BY MONTHS')
fig.show()

# January followed by October are the peak months for investing or buying a share from Apply for better return rate

# Volume of shares by day to see the right month for investing
volume_per_day = apple_df.groupby("day_name")[["Volume"]].sum()
print(volume_per_day)

fig = px.line(volume_per_day, x=volume_per_day.index, y='Volume', title='AVERAGE VOLUME BY DAYS')
fig.show()

# Wednesday followed by Thursday are peak days for investing or buying a share at Apple

# EDA on closing price of stocks
apple_df.columns
sns.boxplot(apple_df['Close'])

sns.boxplot(years_after_2010['Close'])

sns.distplot(apple_df['Close'])

sns.distplot(years_after_2010['Close'])

results = seasonal_decompose(years_after_2010["Close"], model="multiplicative", period=300)
fig = results.plot()
fig.set_size_inches(12, 8)
fig.tight_layout()
plt.show()

# Stationarity series over frames check
df_monthly = years_after_2010.resample("MS").sum()
df_monthly.drop('year', axis =1 , inplace = True)
df_monthly

# Dickey-Fuller Test - if P-Value > 5% then data is in Non stationarity series
dftest = adfuller(df_monthly['Close'], autolag= "AIC")

dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])
for key,value in dftest[4].items():
    dfoutput['Critical Value (%s)'%key] = value
    
print(dfoutput)

def ACF_and_PACF(data):
  fig = plt.figure(figsize = (17,10))
  ax1 = fig.add_subplot(211)
  fig = plot_acf(data , lags = 50 , ax=ax1)
  ax2 = fig.add_subplot(212)
  fig = plot_pacf(data , lags = 50 , ax=ax2)

# Auto correlation and Partial Correlation
ACF_and_PACF(df_monthly['Close'])

df_monthly.shape

df_model=df_monthly['Close']

# Taking 70% train and 30% test
train = df_model[0:95]
test = df_model[95 : 138]

# SARIMA Model Parameters optimization, round-robin algorithm â€” ACF and PACF Plots
# paramiter range order(p, d, q)
min_p = 1; max_p = 2 # min_p must be >1
min_d = 0; max_d = 2
min_q = 0; max_q = 2 

# seasonal_order(sp, sd, sq)
min_sp = 0; max_sp = 1
min_sd = 0; max_sd = 1
min_sq = 0; max_sq = 1

test_pattern = (max_p - min_p +1)*(max_q - min_q + 1)*(max_d - min_d + 1)*(max_sp - min_sp + 1)*(max_sq - min_sq + 1)*(max_sd - min_sd + 1)
print("pattern:", test_pattern)

# seasonal_order and training data
sfq = 12
ts = train

test_results = pd.DataFrame(index=range(test_pattern), columns=["model_parameters", "aic"])
num = 0
for p in range(min_p, max_p + 1):
    for d in range(min_d, max_d + 1):
        for q in range(min_q, max_q + 1):
            for sp in range(min_sp, max_sp + 1):
                for sd in range(min_sd, max_sd + 1):
                    for sq in range(min_sq, max_sq + 1):
                        sarima = sm.tsa.SARIMAX(
                            ts, order=(p, d, q), 
                            seasonal_order=(sp, sd, sq, sfq), 
                            enforce_stationarity = False, 
                            enforce_invertibility = False
                        ).fit()
                        test_results.iloc[num]["model_parameters"] = "order=(" + str(p) + ","+ str(d) + ","+ str(q) + "), seasonal_order=("+ str(sp) + ","+ str(sd) + "," + str(sq) + ")"
                        test_results.iloc[num]["aic"] = sarima.aic
                        print(num,'/', test_pattern-1, test_results.iloc[num]["model_parameters"],  test_results.iloc[num]["aic"] )
                        num = num + 1

# result and AIC
print("best[aic] parameter ********")
print(test_results[test_results.aic == min(test_results.aic)])

test_results.sort_values(by='aic').head(10)

sarimax = sm.tsa.SARIMAX(train, 
                        order=(2, 2, 2),
                        seasonal_order=(0, 1, 1, 4),
                        enforce_stationarity = False,
                        enforce_invertibility = False
                        ).fit(ic='aic', dates=train.index)
print(sarimax.summary())

sarimax_optimization_resid = sarimax.resid # resid check

fig = plt.figure(figsize=(8, 8))

# ACF of resid
ax1 = fig.add_subplot(211)
sm.graphics.tsa.plot_acf(sarimax_optimization_resid, lags=30, ax=ax1) 

# PACF of resid
ax2 = fig.add_subplot(212)
sm.graphics.tsa.plot_pacf(sarimax_optimization_resid, lags=30, ax=ax2) 
plt.show()

sarimax.plot_diagnostics()

# prediction
predict = sarimax.predict('2011-01-01','2022-06-01')

# Visualization
plt.figure(figsize=(10,6))
plt.plot(df_monthly.index, df_monthly["Close"], label="DATASET")
plt.plot(predict.index, predict, label="FUTURE PREDICTION USING SARIMA MODEL", color="orange")
plt.legend()

predict_test = sarimax.predict('2018-12-01','2022-06-01')

# Performance report
mse = mean_squared_error(test, predict_test)
print('MSE: '+str(mse))
mae = mean_absolute_error(test, predict_test)
print('MAE: '+str(mae))
rmse = math.sqrt(mean_squared_error(test, predict_test))
print('RMSE: '+str(rmse))
mape = np.mean(np.abs(test-predict_test )/np.abs(test))
print('MAPE: '+str(mape))

years_after_2010

df_svr=years_after_2010.copy()

df_svr

df_svr.drop(['Open','High','Low','Adj Close','Volume','year','month','day_name'], axis =1, inplace = True)

df_svr

split_date = "2018-01-01"

split_date = "2018-01-01"
data_train = df_svr.loc[df_svr.index <= split_date].copy()
data_test = df_svr.loc[df_svr.index > split_date].copy()

data_train

data_test

res2 = data_test\
.rename(columns= {'Close' : 'TEST SET'})\
.join (data_train.rename(columns= {'Close' : 'TRAINING SET'}), how = 'outer')\
.plot(figsize = (15,5) , title = "Close Price" , style = ".")

def create_features(df, label=None):
    """
    Creates time series features from datetime index
    """
    df['date'] = df.index
    df['dayofweek'] = df['date'].dt.dayofweek
    df['quarter'] = df['date'].dt.quarter
    df['month'] = df['date'].dt.month
    df['year'] = df['date'].dt.year
    df['dayofyear'] = df['date'].dt.dayofyear
    df['dayofmonth'] = df['date'].dt.day
    df['weekofyear'] = df['date'].dt.weekofyear
    
    X = df[['dayofweek','quarter','month','year',
           'dayofyear','dayofmonth','weekofyear']]
    if label:
        y = df[label]
        return X, y
    return X

X_train , y_train = create_features(data_train , label= "Close")
X_test , y_test = create_features(data_test , label= "Close")

X_train

y_train

for k in ['linear' , 'poly' , 'rbf' , 'sigmoid']:
    reg = svm.SVR (kernel=k)
    reg.fit(X_train , y_train)
    result = reg.score(X_train ,y_train)
    print (k , result)
    
# It is shown that linear has the best score

Svr =SVR(kernel= 'linear' , C=1)
Svr.fit(X_train , y_train)

y_predict = Svr.predict(X_test)

# Performance report
mse = mean_squared_error(y_test, y_predict)
print('MSE: '+str(mse))
mae = mean_absolute_error(y_test, y_predict)
print('MAE: '+str(mae))
rmse = math.sqrt(mean_squared_error(y_test, y_predict))
print('RMSE: '+str(rmse))
mape = np.mean(np.abs(y_test- y_predict)/np.abs(y_test))
print('MAPE: '+str(mape))

data_test['Close_pred'] = y_predict
data_all = pd.concat([data_test , data_train] , sort= False)
res2 = data_all[['Close' , 'Close_pred']].plot(figsize = (15,5))

# Results of both SVR and SARIMAX shows very low MAPE

# Forecasting the Closing Price From 2022 to 2023
df_monthly.tail()

# prediction with SARIMA  with better accuarcy
predict = sarimax.predict('2022-06-01','2023-06-01')

# Visualization
plt.figure(figsize=(10,6))
plt.plot(df_monthly.index, df_monthly["Close"], label="DATASET")
plt.plot(predict.index, predict, label="FUTURE PREDICTION USING SARIMA MODEL", color="orange")
plt.legend()

predict